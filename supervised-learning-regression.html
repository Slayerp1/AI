<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Supervised Learning Algorithms ‚Äî Regression Models</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=DM+Mono:wght@400;500&display=swap');
@import url('https://fonts.googleapis.com/css2?family=Syne:wght@400;600;700;800&family=Syne+Mono&display=swap');

:root {
  --bg: #04040a;
  --accent: #7b2fff;
  --hot: #ff2d78;
  --cool: #00e5ff;
  --warm: #ffb800;
  --green: #00ff9d;
  --text: #e0e0f0;
  --muted: #4a4a6a;
  --card: #0a0a14;
  --border: rgba(255,255,255,0.07);
}

* { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: 'Syne', sans-serif;
  overflow-x: hidden;
  line-height: 1.8;
}

body::before {
  content: '';
  position: fixed; inset: 0; z-index: 0;
  background:
    radial-gradient(ellipse 100% 60% at 50% 0%, rgba(123,47,255,0.12) 0%, transparent 60%),
    radial-gradient(ellipse 60% 40% at 0% 100%, rgba(0,229,255,0.06) 0%, transparent 60%);
  pointer-events: none;
}

.wrap { position: relative; z-index: 1; max-width: 1100px; margin: 0 auto; padding: 0 28px; }

nav {
  position: sticky; top: 0; z-index: 100;
  background: rgba(4,4,10,0.9);
  backdrop-filter: blur(24px);
  border-bottom: 1px solid var(--border);
  padding: 14px 28px;
  display: flex; align-items: center; gap: 10px; flex-wrap: wrap;
}
.nav-brand { font-family: 'Bebas Neue', sans-serif; font-size: 20px; color: var(--accent); margin-right: auto; letter-spacing: 2px; }
.nav-pill {
  font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 2px;
  padding: 5px 12px; border-radius: 20px; border: 1px solid rgba(123,47,255,0.4);
  color: rgba(123,47,255,0.8); text-decoration: none; transition: all 0.2s;
}
.nav-pill:hover { background: rgba(123,47,255,0.15); border-color: var(--accent); color: #fff; }

.hero { padding: 90px 0 50px; }
.hero-eyebrow {
  font-family: 'Syne Mono', monospace; font-size: 11px; letter-spacing: 4px;
  color: var(--accent); text-transform: uppercase; margin-bottom: 18px;
  display: flex; align-items: center; gap: 12px;
}
.hero-eyebrow::after { content: ''; flex: 1; height: 1px; background: rgba(123,47,255,0.3); }

.hero h1 {
  font-family: 'Bebas Neue', sans-serif;
  font-size: clamp(48px, 10vw, 100px);
  line-height: 0.92; margin-bottom: 28px;
  color: #fff;
}

.hero-desc { max-width: 750px; font-size: 16px; color: rgba(210,210,240,0.72); line-height: 1.8; margin-bottom: 40px; }

section { padding: 70px 0; border-top: 1px solid var(--border); }
.section-label { font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 4px; color: var(--accent); text-transform: uppercase; margin-bottom: 16px; }
.section-title { font-family: 'Bebas Neue', sans-serif; font-size: clamp(36px, 6vw, 64px); line-height: 1; margin-bottom: 40px; color: #fff; }

.concept-card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 40px;
  margin-bottom: 36px;
  border-left: 4px solid var(--card-accent, var(--accent));
  transition: all 0.3s;
}

.concept-card:hover {
  border-color: var(--card-accent, var(--accent));
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, transparent 100%);
  transform: translateY(-2px);
}

.concept-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 28px;
  color: var(--card-accent, var(--accent));
  margin-bottom: 20px;
  letter-spacing: 1px;
}

.concept-body {
  font-size: 15px;
  color: rgba(210,210,240,0.88);
  line-height: 1.9;
  margin-bottom: 28px;
}

.concept-body p {
  margin-bottom: 18px;
}

.concept-body strong {
  color: #fff;
}

.code-block {
  background: rgba(0,0,0,0.5);
  border: 1px solid rgba(0,229,255,0.2);
  border-radius: 12px;
  padding: 24px;
  margin: 28px 0;
  font-family: 'DM Mono', monospace;
  font-size: 13px;
  color: #fff;
  overflow-x: auto;
  line-height: 1.6;
}

.code-comment { color: #5c7a8a; }
.code-keyword { color: var(--accent); }
.code-string { color: var(--green); }
.code-number { color: var(--warm); }
.code-function { color: var(--cool); }

.teaching-box {
  background: rgba(123,47,255,0.1);
  border-left: 4px solid var(--accent);
  padding: 24px;
  border-radius: 10px;
  margin: 28px 0;
  font-size: 15px;
  color: rgba(210,210,240,0.9);
  line-height: 1.8;
}

.teaching-box strong {
  color: #fff;
}

.comparison-box {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
  gap: 20px;
  margin: 28px 0;
}

.comparison-card {
  background: rgba(0,0,0,0.3);
  border: 1px solid rgba(255,255,255,0.08);
  border-radius: 12px;
  padding: 24px;
  border-left: 3px solid var(--comp-color, var(--accent));
}

.comp-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 16px;
  color: var(--comp-color, var(--accent));
  margin-bottom: 14px;
  letter-spacing: 1px;
}

.comp-content {
  font-size: 13px;
  color: rgba(200,200,220,0.85);
  line-height: 1.8;
}

.impact-box {
  background: linear-gradient(135deg, rgba(123,47,255,0.15) 0%, rgba(0,229,255,0.08) 100%);
  border: 1px solid rgba(123,47,255,0.3);
  border-radius: 14px;
  padding: 32px;
  margin-top: 32px;
}

.impact-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
  gap: 28px;
}

.impact-item {
  padding: 24px;
  background: rgba(0,0,0,0.2);
  border-radius: 10px;
  border-left: 4px solid var(--impact-color, var(--accent));
}

.impact-item-title {
  font-family: 'Syne Mono', monospace;
  font-size: 12px;
  letter-spacing: 2px;
  color: var(--impact-color, var(--accent));
  text-transform: uppercase;
  margin-bottom: 14px;
  font-weight: 600;
}

.impact-item-content {
  font-size: 13px;
  color: rgba(200,200,220,0.88);
  line-height: 1.8;
}

.timeline {
  position: relative;
  padding: 40px 0;
}

.timeline-item {
  padding: 28px;
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  margin-bottom: 24px;
  margin-left: 40px;
  position: relative;
}

.timeline-item::before {
  content: '';
  position: absolute;
  left: -32px;
  top: 32px;
  width: 16px;
  height: 16px;
  background: var(--accent);
  border-radius: 50%;
  border: 3px solid var(--bg);
}

.timeline-item::after {
  content: '';
  position: absolute;
  left: -24px;
  top: 48px;
  width: 2px;
  height: 44px;
  background: rgba(123,47,255,0.3);
}

.timeline-item:last-child::after {
  display: none;
}

.timeline-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 18px;
  color: var(--cool);
  margin-bottom: 10px;
  letter-spacing: 1px;
}

.timeline-desc {
  font-size: 14px;
  color: rgba(200,200,220,0.85);
  line-height: 1.8;
}

.insight-box {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 32px;
  margin: 28px 0;
  border-left: 4px solid var(--insight-color, var(--accent));
}

.insight-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 20px;
  color: var(--insight-color, var(--accent));
  margin-bottom: 16px;
  letter-spacing: 1px;
}

.insight-content {
  font-size: 15px;
  color: rgba(210,210,240,0.88);
  line-height: 1.8;
}

footer {
  padding: 60px 0 40px;
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-text {
  font-family: 'Syne Mono', monospace;
  font-size: 11px;
  letter-spacing: 2px;
  color: var(--muted);
  text-transform: uppercase;
}

@media (max-width: 768px) {
  .hero { padding: 60px 0 40px; }
  section { padding: 50px 0; }
  .impact-grid { grid-template-columns: 1fr; }
  .comparison-box { grid-template-columns: 1fr; }
}
</style>
</head>
<body>

<nav>
  <div class="nav-brand">REGRESSION MODELS</div>
  <a href="#linear" class="nav-pill">Linear</a>
  <a href="#regularized" class="nav-pill">Regularized</a>
  <a href="#advanced" class="nav-pill">Advanced</a>
</nav>

<div class="wrap">

  <section class="hero">
    <div class="hero-eyebrow">Learning to Predict Continuous Values</div>
    <h1>Supervised Learning Algorithms: Regression Models</h1>
    <p class="hero-desc">Regression is the art and science of predicting continuous numerical values from input features. Unlike classification where you choose between discrete categories, regression requires learning the underlying relationship between variables so you can predict values that fall anywhere in a continuous range. The journey from simple linear regression through complex kernel methods represents an escalating toolbox for capturing increasingly sophisticated relationships in data. Understanding these algorithms is not about memorizing formulas but about recognizing that different relationship structures require different modeling approaches. Linear regression assumes straight-line relationships. Polynomial regression captures curved relationships. Regularized approaches prevent overfitting to noisy training data. Kernel methods can handle nonlinear relationships in high-dimensional spaces. This section teaches you to recognize what type of relationship your data exhibits and select the appropriate algorithm to capture it. More importantly, it teaches you the principles underlying these algorithms so you can understand when they work, when they fail, and how to adapt them to your specific problems.</p>
  </section>

  <section id="linear">
    <div class="section-label">The Foundation</div>
    <h2 class="section-title">Linear Regression: Learning Straight-Line Relationships</h2>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">üìà Simple and Multiple Linear Regression</div>
      <div class="concept-body">
        <p>Linear regression represents the most fundamental approach to predicting continuous values. The core idea is beautifully simple: you assume the relationship between input features and output is linear. Given features x, you predict y by computing a weighted sum of those features plus a constant term. The model learns these weights during training by finding the weights that minimize prediction error. This simplicity is deceptive because linear regression is mathematically rich and surprisingly powerful for many real-world problems.</p>

        <p>Simple linear regression involves only one input feature predicting one output. You're fitting a straight line through data points, trying to minimize the vertical distance between observed values and predicted values. The mathematics involves finding the line that minimizes the sum of squared errors, a criterion known as ordinary least squares. While simple in principle, the mathematical framework is elegant and connects directly to probability theory, allowing us to not just make predictions but understand confidence intervals and statistical significance.</p>

        <p>Multiple regression extends this to many input features, fitting a hyperplane through high-dimensional space rather than a line through two dimensions. The mathematics becomes more complex, requiring matrix operations, but the fundamental principle remains identical: find weights that minimize squared prediction error. Multiple regression is where linear regression becomes practical for real-world problems because most predictions depend on multiple factors. A house price depends on size, location, age, number of bedrooms‚Äînot just one feature. Multiple regression handles this naturally, learning how much each feature contributes to the prediction.</p>

        <p>The power of linear regression lies not just in its simplicity but in its interpretability. Each learned weight has a clear meaning: it represents how much the prediction changes for a unit change in that feature, holding other features constant. If the weight for square footage is 100, that means each additional square foot adds 100 dollars to the predicted price. This interpretability is invaluable in many domains where understanding why the model makes predictions is as important as the predictions themselves. A loan officer needs to know why an application was denied. A doctor needs to know what factors drove a diagnosis. Linear regression provides this transparency.</p>
      </div>

      <div class="teaching-box">
        <strong>Understanding linear regression as optimization.</strong> Imagine standing in a landscape where height represents prediction error. You start at a random point with random weights. Your goal is to reach the lowest valley, the point where prediction error is minimized. Linear regression uses calculus to identify the exact location of this lowest point, computing the optimal weights analytically in one step rather than searching gradually. This is the power of linear regression: for this simple problem structure, we can find the perfect solution mathematically. This changes when we add complexity‚Äînonlinearity, regularization, or high-dimensional data‚Äîwhere we must search for good solutions rather than computing perfect ones.
      </div>

      <div class="code-block">
<span class="code-comment"># Simple and multiple linear regression</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LinearRegression
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">import</span> matplotlib.pyplot <span class="code-keyword">as</span> plt

<span class="code-comment"># Simple linear regression: one feature predicting house price</span>
X_simple = np.array([[1000], [1500], [2000], [2500], [3000]])  <span class="code-comment"># Square footage</span>
y = np.array([300000, 400000, 500000, 600000, 700000])  <span class="code-comment"># Prices</span>

<span class="code-comment"># Fit the model: finds weights that minimize prediction error</span>
model_simple = LinearRegression()
model_simple.fit(X_simple, y)

<span class="code-comment"># Model learned: predicted_price = 200 * square_feet + 100000</span>
<span class="code-function">print</span>(<span class="code-string">f"Intercept (base price): ${model_simple.intercept_:,.0f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Coefficient ($/sq ft): ${model_simple.coef_[0]:,.0f}"</span>)

<span class="code-comment"># Make predictions on new data</span>
new_house_sqft = np.array([[2200]])
predicted_price = model_simple.predict(new_house_sqft)
<span class="code-function">print</span>(<span class="code-string">f"Predicted price for 2200 sq ft: ${predicted_price[0]:,.0f}"</span>)

<span class="code-comment"># Multiple linear regression: multiple features predicting price</span>
<span class="code-comment"># Features: square footage, bedrooms, age (in years)</span>
X_multiple = np.array([
    [1000, 3, 10],
    [1500, 4, 5],
    [2000, 4, 20],
    [2500, 5, 2],
    [3000, 5, 15]
])
y = np.array([300000, 400000, 500000, 600000, 700000])

<span class="code-comment"># Fit multiple regression model</span>
model_multiple = LinearRegression()
model_multiple.fit(X_multiple, y)

<span class="code-comment"># Model learned weights for each feature</span>
feature_names = [<span class="code-string">'square_footage'</span>, <span class="code-string">'bedrooms'</span>, <span class="code-string">'age_years'</span>]
<span class="code-keyword">for</span> name, coef <span class="code-keyword">in</span> <span class="code-function">zip</span>(feature_names, model_multiple.coef_):
    <span class="code-function">print</span>(<span class="code-string">f"{name}: {coef:.2f}"</span>)
<span class="code-comment"># Interpretation: each additional square foot adds this much to price, etc.</span>

<span class="code-comment"># R-squared: proportion of variance explained</span>
r2_score = model_multiple.score(X_multiple, y)
<span class="code-function">print</span>(<span class="code-string">f"R¬≤ Score: {r2_score:.3f}"</span>)
<span class="code-comment"># 1.0 = perfect prediction. 0.0 = model explains none of the variation.</span>
      </code-block>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Linear regression is the workhorse of applied statistics and machine learning. Predicting sales from marketing spend, stock prices from economic indicators, test scores from study hours. Any problem where you suspect a linear relationship and need interpretable results.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Linear regression is powerful because it solves the problem analytically. You get optimal weights directly rather than searching. The learned weights are interpretable, telling you exactly how features contribute. The model is simple enough to understand deeply, fast enough to train instantly, and often sufficient for real problems.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Linear regression should be your starting point for any regression problem. Before trying complex models, establish a linear baseline. If relationships appear curved, move to polynomial regression. If data has many features and you suspect overfitting, add regularization. But always start simple.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--warm);">
      <div class="concept-title">üîÑ Polynomial Regression: Capturing Curves</div>
      <div class="concept-body">
        <p>Linear regression assumes straight-line relationships, but many real relationships are curved. Plant growth accelerates then plateaus. Learning curves improve rapidly then level off. Demand might follow a parabolic relationship with price. When you visualize your data and see curved patterns rather than linear ones, you need polynomial regression. The key insight is that polynomial regression is still linear regression, just applied to transformed features. Instead of predicting y from x directly, you predict y from x, x¬≤, x¬≥, and so on. The model still uses linear algebra‚Äîfinding weights that minimize squared error‚Äîbut applied to higher-order polynomial terms.</p>

        <p>The challenge with polynomial regression is choosing the polynomial degree. Degree 1 (linear) might underfit by missing the curve. Degree 2 (quadratic) might fit perfectly. Degree 3 (cubic) might start overfitting. Degree 10 will almost certainly memorize noise in the training data rather than learning the true underlying pattern. This is where the bias-variance tradeoff becomes visible. Lower degree polynomials have high bias (miss the true pattern) and low variance. Higher degree polynomials have low bias (capture complex patterns) but high variance (overfit to noise). The optimal degree is somewhere in the middle, found through validation. This is why training on a full dataset then evaluating on held-out test data is critical‚Äîit prevents you from fooling yourself about a model that merely memorizes training data.</p>

        <p>Polynomial regression illustrates an important principle in machine learning: feature engineering matters as much as algorithm selection. By creating polynomial features, you expand the space of functions the model can represent. The algorithm remains linear regression‚Äîjust applied to different features. This principle extends broadly: the features you give to an algorithm determine what relationships it can possibly learn. No algorithm can learn a relationship that isn't represented in the features you provide.</p>
      </div>

      <div class="code-block">
<span class="code-comment"># Polynomial regression: fitting curves</span>
<span class="code-keyword">from</span> sklearn.preprocessing <span class="code-keyword">import</span> PolynomialFeatures
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LinearRegression
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Data with curved relationship (quadratic)</span>
X = np.linspace(<span class="code-number">0</span>, <span class="code-number">10</span>, <span class="code-number">20</span>).reshape(-<span class="code-number">1</span>, <span class="code-number">1</span>)
y = <span class="code-number">2</span> * X.ravel()**<span class="code-number">2</span> - <span class="code-number">3</span> * X.ravel() + <span class="code-number">5</span> + np.random.normal(<span class="code-number">0</span>, <span class="code-number">5</span>, <span class="code-number">20</span>)
<span class="code-comment"># True underlying relationship: y = 2x¬≤ - 3x + 5</span>

<span class="code-comment"># Try different polynomial degrees</span>
train_errors = []
test_errors = []

<span class="code-keyword">for</span> degree <span class="code-keyword">in</span> <span class="code-function">range</span>(<span class="code-number">1</span>, <span class="code-number">8</span>):
    <span class="code-comment"># Create polynomial features</span>
    poly_features = PolynomialFeatures(degree=degree, include_bias=<span class="code-keyword">False</span>)
    X_poly = poly_features.fit_transform(X)
    
    <span class="code-comment"># Train linear regression on polynomial features</span>
    model = LinearRegression()
    model.fit(X_poly, y)
    
    <span class="code-comment"># Evaluate</span>
    train_error = mean_squared_error(y, model.predict(X_poly))
    <span class="code-comment"># Test error on held-out data (not shown for brevity)</span>
    
    train_errors.append(train_error)
    
    <span class="code-function">print</span>(<span class="code-string">f"Degree {degree}: Training MSE = {train_error:.2f}"</span>)

<span class="code-comment"># Degree 2 should have lowest error (matches true quadratic relationship)</span>
<span class="code-comment"># Degree 1 underfits (linear can't capture the curve)</span>
<span class="code-comment"># Degree 7 overfits (memorizes noise)</span>
      </code-block>
    </div>

  </section>

  <section id="regularized">
    <div class="section-label">Controlling Complexity</div>
    <h2 class="section-title">Regularized Regression: Ridge, Lasso, and Elastic Net</h2>

    <div class="concept-card" style="--card-accent: var(--green);">
      <div class="concept-title">üõ°Ô∏è Understanding Regularization Fundamentally</div>
      <div class="concept-body">
        <p>Regularization is one of the most important concepts in machine learning because it directly addresses the fundamental bias-variance tradeoff. The core problem it solves is this: standard linear regression finds weights that minimize squared prediction error on training data, period. If the training data is noisy, the model learns to fit that noise along with the true pattern. If you have many features, the model might distribute predictive power across many redundant features rather than focusing on the few that truly matter. Regularization adds a penalty term that discourages the model from learning certain solutions, even if those solutions fit the training data better.</p>

        <p>Ridge regression adds a penalty proportional to the square of the weights, a technique called L2 regularization. The model now minimizes prediction error plus a penalty for having large weights. This encourages the model to use all features but keep their contributions small. Large weights are only learned if they genuinely help predict better. The strength of regularization is controlled by a parameter Œª (lambda): larger Œª means stronger penalty, forcing weights closer to zero, reducing model complexity and reducing variance at the cost of increased bias. Smaller Œª means weaker penalty, allowing weights to grow larger, fitting training data better but risking overfitting. This Œª parameter must be tuned, typically using cross-validation to find the value that minimizes error on held-out data.</p>

        <p>Lasso regression uses L1 regularization, penalizing the absolute value of weights rather than their squares. This seemingly small mathematical difference has a profound consequence: Lasso tends to drive weights to exactly zero rather than just making them small. This performs automatic feature selection, zeroing out unimportant features entirely. When you have many features and suspect only a few matter, Lasso discovers which ones. Ridge keeps all features but shrinks their contributions. Lasso selects a subset of features with nonzero weights. Elastic Net combines both approaches, using a weighted mixture of L1 and L2 penalties, giving you flexibility to get properties of both.</p>

        <p>The intuition behind regularization is that simpler models‚Äîthose using fewer features or smaller weights‚Äîgeneralize better. This reflects the principle of Occam's Razor: among explanations that fit the data equally well, prefer the simplest. By penalizing complexity, regularization implements this principle mathematically. The art is choosing how much to penalize. Too much regularization (high Œª) and the model becomes too simple, underfitting by ignoring real patterns. Too little (low Œª) and regularization barely helps, leaving the model prone to overfitting. Cross-validation reveals the sweet spot where validation error is minimized.</p>
      </div>

      <div class="teaching-box">
        <strong>Regularization as philosophical principle in mathematics.</strong> Imagine two explanations for data: one uses five features with weights of 100 each, another uses fifty features with weights of 1 each. Both fit training data identically. Which should you prefer? The first is simpler, easier to understand, and likely more robust to noise. Regularization encodes this preference mathematically. It says: "I'll let you fit training data as well as you want, but I'm charging you for complexity." How much you charge‚Äîthe regularization strength‚Äîdetermines whether you prefer simple or complex explanations. This mirrors human learning: we prefer simple rules that explain observations rather than elaborate ones that merely memorize.
      </div>

      <div class="code-block">
<span class="code-comment"># Ridge, Lasso, and Elastic Net regression</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> Ridge, Lasso, ElasticNet
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> RidgeCV, LassoCV
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error

<span class="code-comment"># Data with many features, some redundant</span>
X_train = ...  <span class="code-comment"># Many features</span>
y_train = ...
X_test = ...
y_test = ...

<span class="code-comment"># Ridge regression: L2 regularization (penalty on weight magnitude)</span>
<span class="code-comment"># Keep all features but shrink weights of unimportant ones</span>
ridge_model = Ridge(alpha=<span class="code-number">1.0</span>)  <span class="code-comment"># alpha controls regularization strength</span>
ridge_model.fit(X_train, y_train)
ridge_error = mean_squared_error(y_test, ridge_model.predict(X_test))
<span class="code-function">print</span>(<span class="code-string">f"Ridge MSE: {ridge_error:.4f}"</span>)

<span class="code-comment"># Lasso regression: L1 regularization (penalty on absolute weight)</span>
<span class="code-comment"># Automatically performs feature selection: sets unimportant weights to zero</span>
lasso_model = Lasso(alpha=<span class="code-number">0.1</span>)
lasso_model.fit(X_train, y_train)
lasso_error = mean_squared_error(y_test, lasso_model.predict(X_test))
nonzero_features = np.sum(lasso_model.coef_ != <span class="code-number">0</span>)
<span class="code-function">print</span>(<span class="code-string">f"Lasso MSE: {lasso_error:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Non-zero features: {nonzero_features} / {X_train.shape[1]}"</span>)

<span class="code-comment"># Find optimal alpha using cross-validation</span>
<span class="code-comment"># RidgeCV tries multiple alpha values and picks the best one</span>
ridge_cv = RidgeCV(alphas=[<span class="code-number">0.1</span>, <span class="code-number">1.0</span>, <span class="code-number">10.0</span>, <span class="code-number">100.0</span>])
ridge_cv.fit(X_train, y_train)
<span class="code-function">print</span>(<span class="code-string">f"Best alpha: {ridge_cv.alpha_}"</span>)

<span class="code-comment"># Elastic Net: combines L1 and L2 regularization</span>
<span class="code-comment"># l1_ratio=0 is pure Ridge, l1_ratio=1 is pure Lasso</span>
elastic_model = ElasticNet(alpha=<span class="code-number">0.5</span>, l1_ratio=<span class="code-number">0.5</span>)
elastic_model.fit(X_train, y_train)
elastic_error = mean_squared_error(y_test, elastic_model.predict(X_test))
<span class="code-function">print</span>(<span class="code-string">f"Elastic Net MSE: {elastic_error:.4f}"</span>)
      </code-block>
    </div>

  </section>

  <section id="advanced">
    <div class="section-label">Beyond Linear Relationships</div>
    <h2 class="section-title">Support Vector Regression and Kernel Methods</h2>

    <div class="concept-card" style="--card-accent: var(--hot);">
      <div class="concept-title">üåÄ Kernel Methods: Learning in High Dimensions</div>
      <div class="concept-body">
        <p>Support vector regression and kernel methods represent a shift from learning linear relationships in the original feature space to learning complex nonlinear relationships in a transformed space. The fundamental insight is this: you can apply linear regression to a transformed version of your features. If you have features x‚ÇÅ and x‚ÇÇ, you can create new features like x‚ÇÅx‚ÇÇ, x‚ÇÅ¬≤, x‚ÇÇ¬≤ and apply linear regression to these new features. This is equivalent to polynomial regression. Kernel methods generalize this idea by allowing transformations to very high dimensions, even infinite dimensions, where a linear boundary can separate complex nonlinear patterns in the original space.</p>

        <p>The mathematical trick that makes this tractable is the kernel trick. Computing x‚ÇÅ¬≤, x‚ÇÅ¬≤x‚ÇÇ, etc. explicitly becomes expensive in high dimensions. The kernel trick computes the dot product of transformed features without explicitly computing the transformation. For instance, the polynomial kernel computes the similarity between two data points as if they were transformed to polynomial features, but without creating those features. This allows support vector regression to learn in incredibly high-dimensional transformed spaces while remaining computationally tractable. Different kernels encode different assumptions about what transformations are useful. A polynomial kernel assumes polynomial relationships. An RBF (radial basis function) kernel assumes local relationships where nearby points are similar. A sigmoid kernel approximates neural network behavior.</p>

        <p>Support vector regression specifically uses the principle of large margins: it finds a function that fits the data while keeping the margin between predicted and actual values as consistent as possible. This is related to support vector machines for classification but adapted for regression. The model is insensitive to points that are within a specified tolerance of the prediction (epsilon), only penalizing large errors. This makes SVR robust to outliers. A single outlier that would heavily influence linear regression gets largely ignored by SVR if it's within the tolerance band. This robustness plus the ability to capture complex nonlinear relationships makes SVR powerful for many problems, though less interpretable than linear models since learned decision boundaries exist in transformed high-dimensional spaces.</p>

        <p>Understanding kernel methods requires embracing that we're finding linear relationships in transformed space, not linear relationships in the original space. A kernel creates a similarity measure between data points. Linear regression in that kernel space corresponds to a nonlinear function in the original space. This perspective unifies many algorithms: neural networks implicitly learn transformations through layers, kernel methods explicitly apply predetermined transformations, and feature engineering manually creates useful transformations. The principle is consistent: find weights for transformed features that minimize error.</p>
      </div>

      <div class="teaching-box">
        <strong>Kernels as similarity measures.</strong> Imagine you're trying to separate apples from oranges by color alone. In RGB color space, they might overlap. But if you transform to a space measuring "orange-ness" (how much red and yellow) versus "apple-ness" (how much red and green), they become separable. The kernel computes this similarity implicitly. A linear classifier in this transformed space becomes a nonlinear classifier in color space. Kernel methods do this automatically: they find the transformation where linear separation becomes possible, then use linear methods in that transformed space. This is why kernel methods are so powerful‚Äîthey combine the tractability of linear methods with the expressiveness of nonlinear transformations.
      </teaching-box>

      <div class="code-block">
<span class="code-comment"># Support Vector Regression with different kernels</span>
<span class="code-keyword">from</span> sklearn.svm <span class="code-keyword">import</span> SVR
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error, r2_score

<span class="code-comment"># Data with nonlinear relationship</span>
X_train = ...  <span class="code-comment"># Nonlinear problem</span>
y_train = ...
X_test = ...
y_test = ...

<span class="code-comment"># SVR with linear kernel (effectively linear regression)</span>
svr_linear = SVR(kernel=<span class="code-string">'linear'</span>, C=<span class="code-number">100</span>)
svr_linear.fit(X_train, y_train)
linear_r2 = r2_score(y_test, svr_linear.predict(X_test))

<span class="code-comment"># SVR with polynomial kernel (can learn polynomial relationships)</span>
svr_poly = SVR(kernel=<span class="code-string">'poly'</span>, degree=<span class="code-number">3</span>, C=<span class="code-number">100</span>)
svr_poly.fit(X_train, y_train)
poly_r2 = r2_score(y_test, svr_poly.predict(X_test))

<span class="code-comment"># SVR with RBF kernel (can learn complex nonlinear relationships)</span>
<span class="code-comment"># RBF uses local neighborhoods: similar inputs give similar outputs</span>
svr_rbf = SVR(kernel=<span class="code-string">'rbf'</span>, gamma=<span class="code-string">'scale'</span>, C=<span class="code-number">100</span>)
svr_rbf.fit(X_train, y_train)
rbf_r2 = r2_score(y_test, svr_rbf.predict(X_test))

<span class="code-function">print</span>(<span class="code-string">f"Linear kernel R¬≤: {linear_r2:.3f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Polynomial kernel R¬≤: {poly_r2:.3f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"RBF kernel R¬≤: {rbf_r2:.3f}"</span>)
<span class="code-comment"># RBF typically better for nonlinear problems</span>

<span class="code-comment"># Isotonic regression: monotonic but nonlinear</span>
<span class="code-keyword">from</span> sklearn.isotonic <span class="code-keyword">import</span> IsotonicRegression

<span class="code-comment"># Assumes monotonic relationship: output always increases with input</span>
iso_model = IsotonicRegression()
iso_model.fit(X_train, y_train)
iso_r2 = r2_score(y_test, iso_model.predict(X_test))
<span class="code-function">print</span>(<span class="code-string">f"Isotonic regression R¬≤: {iso_r2:.3f}"</span>)
<span class="code-comment"># Useful when you know output monotonically changes with input</span>
      </code-block>
    </div>

  </section>

  <section>
    <div class="section-label">Choosing Your Regression Model</div>
    <h2 class="section-title">A Decision Framework for Algorithm Selection</h2>

    <div class="timeline">
      <div class="timeline-item">
        <div class="timeline-title">Start with Exploratory Visualization</div>
        <div class="timeline-desc">Plot features against the target variable. Do relationships look linear or curved? Are there clusters of data? Are there outliers? This visual exploration should inform your algorithm choice. Linear relationships suggest linear regression. Curved relationships suggest polynomial regression or kernel methods. Clusters suggest the data might have substructure you should model.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Start Simple with Linear Regression</div>
        <div class="timeline-desc">Establish a linear baseline first. Train linear regression and evaluate on held-out test data. This gives you a simple benchmark to beat. Linear regression is fast, interpretable, and often sufficient. Only move to complex models if linear regression leaves substantial error. This prevents unnecessary complexity.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Check for Overfitting</div>
        <div class="timeline-desc">Compare training error to validation error. If they're similar, the model generalizes well. If training error is much lower, the model is overfitting. Overfitting suggests you need regularization. Try Ridge or Lasso regression with cross-validation to find the optimal regularization strength.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Address Nonlinearity When Needed</div>
        <div class="timeline-desc">If regularized linear models still leave significant error and visualization shows curved relationships, try polynomial features or kernel methods. Polynomial regression is interpretable if degree is low. SVR with RBF kernel can capture complex nonlinearity but becomes a black box. Choose based on interpretability needs.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Validate Feature Importance</div>
        <div class="timeline-desc">If you use Lasso regression, examine which features have nonzero weights‚Äîthese are the selected important ones. Linear regression weights show how much each feature contributes. Understanding feature importance helps verify the model makes sense and can inform decisions about which data to collect or engineering.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Evaluate on Unseen Test Data</div>
        <div class="timeline-desc">Never evaluate model quality on data used for training or hyperparameter tuning. Always use a held-out test set. This gives you honest estimate of real-world performance. Test set error reveals whether your model will actually work when deployed.</div>
      </div>
    </div>

  </section>

  <section>
    <div class="section-label">Core Wisdom</div>
    <h2 class="section-title">Principles for Effective Regression Modeling</h2>

    <div class="insight-box" style="--insight-color: var(--cool);">
      <div class="insight-title">Simplicity is Strength: Start with Linear Regression</div>
      <div class="insight-content">Linear regression should be your default first approach for regression problems. It's interpretable, trainable in closed form, and often sufficient. Many problems that seem to require complex models actually solve well with linear regression after proper feature engineering. Don't jump to complex models prematurely. Establish a linear baseline first, then move to more complex approaches only if linear regression leaves substantial error. This disciplined approach prevents unnecessary complexity and keeps your models understandable.</div>
    </div>

    <div class="insight-box" style="--insight-color: var(--green);">
      <div class="insight-title">Regularization Controls the Bias-Variance Tradeoff</div>
      <div class="insight-content">Regularization is not a band-aid fix for overfitting. It's a fundamental tool for controlling model complexity. By penalizing weights, regularization encourages simpler explanations. The regularization strength must be tuned through cross-validation to balance bias and variance. Too much regularization (high Œª) underfits by ignoring real patterns. Too little leaves the model prone to overfitting. The optimal strength is problem-dependent, found through validation. Understanding this tradeoff transforms you from someone who applies regularization blindly to someone who uses it purposefully.</div>
    </div>

    <div class="insight-box" style="--insight-color: var(--hot);">
      <div class="insight-title">Feature Engineering Matters More Than Algorithm Choice</div>
      <div class="insight-content">The quality of features you provide determines what patterns the algorithm can possibly learn. Given good features, simple algorithms work well. Given poor features, even complex algorithms struggle. Invest time in understanding your data, creating meaningful features, and engineering transformations that capture domain knowledge. A sophisticated algorithm applied to weak features loses to simple linear regression applied to engineered features. This principle applies across all machine learning: algorithms work with what you give them. The best algorithms cannot overcome poor feature engineering.</div>
    </div>

    <div class="insight-box" style="--insight-color: var(--accent);">
      <div class="insight-title">Interpretability Has Value Beyond Academic Interest</div>
      <div class="insight-content">Linear models are interpretable: you can see exactly how each feature contributes to predictions. Kernel methods and complex nonlinear models are often black boxes. In many real applications, interpretability matters. A loan officer needs to explain why an application was denied. A doctor needs to understand what clinical factors drove a diagnosis. A business leader needs to understand what drives revenue. When interpretability matters, prefer linear models even if they sacrifice some accuracy. Unexplainably accurate predictions are useless when you need to justify decisions or understand causality.</div>
    </div>

  </section>

</div>

<footer>
  <p class="footer-text">Supervised Learning Algorithms ‚Äî Regression Models ‚Äî Building Effective Prediction Systems</p>
</footer>

</body>
</html>
