<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Linear Algebra Fundamentals ‚Äî The Math Behind AI</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=DM+Mono:wght@400;500&family=Clash+Display:wght@400;600;700&display=swap');
@import url('https://fonts.googleapis.com/css2?family=Syne:wght@400;600;700;800&family=Syne+Mono&display=swap');

:root {
  --bg: #04040a;
  --accent: #7b2fff;
  --hot: #ff2d78;
  --cool: #00e5ff;
  --warm: #ffb800;
  --green: #00ff9d;
  --text: #e0e0f0;
  --muted: #4a4a6a;
  --card: #0a0a14;
  --border: rgba(255,255,255,0.07);
}

* { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: 'Syne', sans-serif;
  overflow-x: hidden;
  line-height: 1.7;
}

/* Starfield bg */
body::before {
  content: '';
  position: fixed; inset: 0; z-index: 0;
  background:
    radial-gradient(ellipse 100% 60% at 50% 0%, rgba(123,47,255,0.12) 0%, transparent 60%),
    radial-gradient(ellipse 60% 40% at 0% 100%, rgba(0,229,255,0.06) 0%, transparent 60%);
  pointer-events: none;
}

.wrap { position: relative; z-index: 1; max-width: 1100px; margin: 0 auto; padding: 0 28px; }

/* NAV */
nav {
  position: sticky; top: 0; z-index: 100;
  background: rgba(4,4,10,0.9);
  backdrop-filter: blur(24px);
  border-bottom: 1px solid var(--border);
  padding: 14px 28px;
  display: flex; align-items: center; gap: 10px; flex-wrap: wrap;
}
.nav-brand { font-family: 'Bebas Neue', sans-serif; font-size: 20px; color: var(--accent); margin-right: auto; letter-spacing: 2px; }
.nav-pill {
  font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 2px;
  padding: 5px 12px; border-radius: 20px; border: 1px solid rgba(123,47,255,0.4);
  color: rgba(123,47,255,0.8); text-decoration: none; transition: all 0.2s;
}
.nav-pill:hover, .nav-pill.active { background: rgba(123,47,255,0.15); border-color: var(--accent); color: #fff; }

/* HERO */
.hero { padding: 90px 0 50px; }
.hero-eyebrow {
  font-family: 'Syne Mono', monospace; font-size: 11px; letter-spacing: 4px;
  color: var(--accent); text-transform: uppercase; margin-bottom: 18px;
  display: flex; align-items: center; gap: 12px;
}
.hero-eyebrow::after { content: ''; flex: 1; height: 1px; background: rgba(123,47,255,0.3); }

.hero h1 {
  font-family: 'Bebas Neue', sans-serif;
  font-size: clamp(48px, 10vw, 100px);
  line-height: 0.92; margin-bottom: 28px;
  color: #fff;
}

.hero-desc { max-width: 700px; font-size: 16px; color: rgba(210,210,240,0.72); line-height: 1.8; margin-bottom: 40px; }

/* SECTION */
section { padding: 70px 0; border-top: 1px solid var(--border); }
.section-label { font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 4px; color: var(--accent); text-transform: uppercase; margin-bottom: 16px; }
.section-title { font-family: 'Bebas Neue', sans-serif; font-size: clamp(36px, 6vw, 64px); line-height: 1; margin-bottom: 36px; color: #fff; }

/* CONCEPT CARD */
.concept-card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 32px;
  margin-bottom: 28px;
  border-left: 4px solid var(--card-accent, var(--accent));
  transition: all 0.3s;
}

.concept-card:hover {
  border-color: var(--card-accent, var(--accent));
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, transparent 100%);
  transform: translateY(-2px);
}

.concept-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 28px;
  color: var(--card-accent, var(--accent));
  margin-bottom: 16px;
  letter-spacing: 1px;
}

.concept-description {
  font-size: 15px;
  color: rgba(210,210,240,0.85);
  line-height: 1.9;
  margin-bottom: 24px;
  background: rgba(0,0,0,0.2);
  padding: 18px;
  border-radius: 10px;
  border-left: 3px solid var(--card-accent, var(--accent));
}

.concept-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
  gap: 20px;
  margin-bottom: 30px;
}

.info-box {
  background: rgba(255,255,255,0.03);
  padding: 20px;
  border-radius: 12px;
  border: 1px solid rgba(255,255,255,0.05);
}

.info-box-title {
  font-family: 'Syne Mono', monospace;
  font-size: 11px;
  letter-spacing: 2px;
  color: var(--card-accent, var(--accent));
  text-transform: uppercase;
  margin-bottom: 12px;
  font-weight: 600;
}

.info-box-content {
  font-size: 13px;
  color: rgba(200,200,220,0.9);
  line-height: 1.8;
}

/* VISUAL EXAMPLE */
.visual-example {
  background: rgba(0,0,0,0.3);
  border: 1px solid rgba(255,255,255,0.08);
  border-radius: 12px;
  padding: 24px;
  margin: 24px 0;
  font-family: 'DM Mono', monospace;
  font-size: 13px;
  color: var(--cool);
  overflow-x: auto;
}

.math-notation {
  background: rgba(123,47,255,0.1);
  padding: 16px;
  border-radius: 8px;
  border-left: 3px solid var(--accent);
  margin: 20px 0;
  font-family: 'DM Mono', monospace;
  color: #fff;
}

/* WHY/WHEN/WHERE BOX */
.impact-box {
  background: linear-gradient(135deg, rgba(123,47,255,0.15) 0%, rgba(0,229,255,0.08) 100%);
  border: 1px solid rgba(123,47,255,0.3);
  border-radius: 14px;
  padding: 28px;
  margin-top: 28px;
}

.impact-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
  gap: 24px;
}

.impact-item {
  padding: 20px;
  background: rgba(0,0,0,0.2);
  border-radius: 10px;
  border-left: 4px solid var(--impact-color, var(--accent));
}

.impact-item-title {
  font-family: 'Syne Mono', monospace;
  font-size: 12px;
  letter-spacing: 2px;
  color: var(--impact-color, var(--accent));
  text-transform: uppercase;
  margin-bottom: 12px;
  font-weight: 600;
}

.impact-item-content {
  font-size: 13px;
  color: rgba(200,200,220,0.85);
  line-height: 1.8;
}

/* MATRIX VISUALIZATION */
.matrix-visual {
  background: rgba(0,0,0,0.4);
  padding: 20px;
  border-radius: 10px;
  margin: 20px 0;
  border: 1px solid rgba(0,229,255,0.2);
}

.matrix-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(40px, 1fr));
  gap: 8px;
  max-width: 300px;
  margin: 16px 0;
}

.matrix-cell {
  background: rgba(123,47,255,0.2);
  border: 1px solid rgba(123,47,255,0.4);
  padding: 12px;
  text-align: center;
  border-radius: 6px;
  font-family: 'DM Mono', monospace;
  font-size: 12px;
  color: var(--cool);
  font-weight: 600;
}

/* TIMELINE */
.timeline {
  position: relative;
  padding: 40px 0;
}

.timeline-item {
  padding: 24px;
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  margin-bottom: 20px;
  margin-left: 40px;
  position: relative;
}

.timeline-item::before {
  content: '';
  position: absolute;
  left: -32px;
  top: 30px;
  width: 16px;
  height: 16px;
  background: var(--accent);
  border-radius: 50%;
  border: 3px solid var(--bg);
}

.timeline-item::after {
  content: '';
  position: absolute;
  left: -24px;
  top: 46px;
  width: 2px;
  height: 40px;
  background: rgba(123,47,255,0.3);
}

.timeline-item:last-child::after {
  display: none;
}

.timeline-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 18px;
  color: var(--cool);
  margin-bottom: 8px;
  letter-spacing: 1px;
}

.timeline-desc {
  font-size: 13px;
  color: rgba(200,200,220,0.8);
}

/* FOOTER */
footer {
  padding: 60px 0 40px;
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-text {
  font-family: 'Syne Mono', monospace;
  font-size: 11px;
  letter-spacing: 2px;
  color: var(--muted);
  text-transform: uppercase;
}

/* RESPONSIVE */
@media (max-width: 768px) {
  .hero { padding: 60px 0 40px; }
  section { padding: 50px 0; }
  .concept-grid { grid-template-columns: 1fr; }
  .impact-grid { grid-template-columns: 1fr; }
}

</style>
</head>
<body>

<nav>
  <div class="nav-brand">LINEAR ALGEBRA</div>
  <a href="#vectors" class="nav-pill">Vectors</a>
  <a href="#matrices" class="nav-pill">Matrices</a>
  <a href="#decomposition" class="nav-pill">Decomposition</a>
  <a href="#applications" class="nav-pill">Applications</a>
</nav>

<div class="wrap">

  <!-- HERO -->
  <section class="hero">
    <div class="hero-eyebrow">The Mathematical Foundation</div>
    <h1>Linear Algebra Fundamentals</h1>
    <p class="hero-desc">Linear algebra is the language of machine learning and artificial intelligence. Every neural network, every computer vision system, every recommendation engine relies on the concepts you're about to learn. This is not abstract mathematics‚Äîit's the concrete tool that powers AI.</p>
  </section>

  <!-- VECTORS -->
  <section id="vectors">
    <div class="section-label">Core Building Block</div>
    <h2 class="section-title">Vectors and Vector Spaces</h2>

    <div class="concept-card" style="--card-accent: var(--green);">
      <div class="concept-title">üìç What is a Vector?</div>
      <div class="concept-description">
        A vector is an ordered list of numbers. Think of it as a direction and magnitude in space. In machine learning, vectors represent almost everything: an image as pixel values, a word as embeddings, a person's preferences as rating values.
      </div>
      
      <div class="concept-grid">
        <div class="info-box">
          <div class="info-box-title">Geometric View</div>
          <div class="info-box-content">An arrow pointing from origin to a point in space. The vector [3, 4] means "go 3 units right, 4 units up."</div>
        </div>
        <div class="info-box">
          <div class="info-box-title">Algebraic View</div>
          <div class="info-box-content">A column of numbers: [3, 4, 2, 7]. Used to represent features, data points, or weights in neural networks.</div>
        </div>
      </div>

      <div class="visual-example">
Vector v = [3, 4]
Magnitude (length): ‚àö(3¬≤ + 4¬≤) = ‚àö25 = 5
Direction: 53.13¬∞ from horizontal axis
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Every data point in machine learning is a vector. Images as flattened pixel vectors, text as word embeddings, audio as spectrogram vectors.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Vectors let us represent high-dimensional data mathematically. A 256√ó256 image becomes a vector of 65,536 numbers we can manipulate algebraically.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Always. Vectors are the fundamental unit of data representation. If you're doing any ML, you're working with vectors whether you realize it or not.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">‚ûï Vector Operations</div>
      <div class="concept-description">
        Vectors can be added, scaled, and combined. These operations are the basic arithmetic of machine learning. When a neural network processes data, it's doing millions of vector operations per second.
      </div>

      <div class="visual-example">
Vector Addition:
v‚ÇÅ = [1, 2]  +  v‚ÇÇ = [3, 4]  =  [4, 6]

Scalar Multiplication:
2 √ó [3, 4] = [6, 8]

Dot Product (most important):
[1, 2] ¬∑ [3, 4] = 1√ó3 + 2√ó4 = 11
      </div>

      <div class="info-box">
        <div class="info-box-title">The Dot Product: The Most Important Operation</div>
        <div class="info-box-content">The dot product measures how aligned two vectors are. If vectors point in same direction: large positive number. Opposite: large negative number. Perpendicular: zero. This single operation is used in every neural network layer.</div>
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Neural network layers compute y = Wx where W is weights matrix and x is input vector. Every neuron computes a dot product between inputs and weights.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Dot product similarity is how neural networks learn patterns. High similarity between input and weight vectors means that neuron activates strongly.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Every single forward pass through a neural network. Computing attention scores in transformers. Calculating similarities between embeddings.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- MATRICES -->
  <section id="matrices">
    <div class="section-label">Data Organization</div>
    <h2 class="section-title">Matrices and Matrix Operations</h2>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">üî≤ What is a Matrix?</div>
      <div class="concept-description">
        A matrix is a 2D grid of numbers. You can think of it as multiple vectors stacked together, or as a table of data. In machine learning, matrices represent weights in neural networks, feature sets of multiple data points, or transformations between spaces.
      </div>

      <div class="matrix-visual">
        <div>A 3√ó3 Matrix:</div>
        <div class="matrix-grid" style="grid-template-columns: repeat(3, 1fr);">
          <div class="matrix-cell">1</div>
          <div class="matrix-cell">2</div>
          <div class="matrix-cell">3</div>
          <div class="matrix-cell">4</div>
          <div class="matrix-cell">5</div>
          <div class="matrix-cell">6</div>
          <div class="matrix-cell">7</div>
          <div class="matrix-cell">8</div>
          <div class="matrix-cell">9</div>
        </div>
        <div style="margin-top: 16px; font-size: 13px; color: rgba(200,200,220,0.8);">
          In neural networks: rows = features, columns = samples. Or rows = weights, columns = destinations.
        </div>
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">The weights of every neural network layer are stored as matrices. A dense layer with 100 inputs and 50 outputs uses a 100√ó50 weight matrix.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Matrices allow us to perform transformations on entire batches of data efficiently. One matrix multiplication can transform 1000 data points simultaneously.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Continuously. Matrix multiplication is the core operation that GPUs are optimized for. Most neural network time is spent doing matrix operations.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--warm);">
      <div class="concept-title">‚úñÔ∏è Matrix Multiplication</div>
      <div class="concept-description">
        Matrix multiplication combines two matrices to produce a third. This is not element-wise multiplication‚Äîit's a more complex operation that's fundamental to how neural networks process data. Every forward pass through a neural network is just matrix multiplication chained together.
      </div>

      <div class="visual-example">
Matrix A (2√ó3)    √ó    Matrix B (3√ó2)    =    Result C (2√ó2)
[1 2 3]                [7 8]                  [1√ó7+2√ó9+3√ó11   1√ó8+2√ó10+3√ó12]
[4 5 6]          √ó     [9 10]         =      [4√ó7+5√ó9+6√ó11   4√ó8+5√ó10+6√ó12]
                       [11 12]

Result: [[58 64], [139 154]]

Key rule: (m√ón) √ó (n√óp) = (m√óp)
The inner dimensions must match!
      </div>

      <div class="info-box">
        <div class="info-box-title">Why This Matters for Neural Networks</div>
        <div class="info-box-content">Neural network forward pass: output = input √ó weight_matrix. If your input is (batch_size √ó input_features) and weights are (input_features √ó output_features), result is (batch_size √ó output_features). This is exactly what we want!</div>
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Neural network forward pass (input √ó weights), attention mechanism (queries √ó keys), embedding lookups, convolutional operations.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Matrix multiplication is the ONLY operation a neural network does during forward pass (besides non-linearities). Everything is matrix multiplication.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Continuously. Modern GPUs are specifically designed to do matrix multiplication extremely fast. This is why training neural networks benefits so much from GPU acceleration.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--accent);">
      <div class="concept-title">üîÑ Matrix Transpose, Inverse, and Determinant</div>
      <div class="concept-description">
        Matrices have special properties and operations that help us solve systems of equations, understand invertibility, and optimize computations. These are tools you'll use frequently in machine learning.
      </div>

      <div class="visual-example">
Matrix Transpose (flip rows and columns):
A = [1 2 3]    ‚Üí    A^T = [1 4]
    [4 5 6]               [2 5]
                          [3 6]

Determinant (numerical property):
For 2√ó2: det([a b; c d]) = ad - bc

Inverse (A^-1 √ó A = Identity):
Only exists for square matrices with non-zero determinant
A^-1 is the matrix that "undoes" A
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Transpose in attention (transpose keys), in backprop (transpose gradients), in data reshaping. Inverse in solving linear systems, optimization algorithms.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Transpose allows us to reshape computations efficiently. Inverse lets us solve equations. Determinant tells us if a matrix is invertible and how much it scales volumes.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Less frequently than multiplication, but essential. Transpose is nearly free computationally. Inverse is expensive but sometimes necessary for mathematical analysis.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- EIGENVALUES AND EIGENVECTORS -->
  <section>
    <div class="section-label">Matrix Properties</div>
    <h2 class="section-title">Eigenvalues and Eigenvectors</h2>

    <div class="concept-card" style="--card-accent: var(--hot);">
      <div class="concept-title">üîë The Key Insight</div>
      <div class="concept-description">
        For many matrices, there exist special vectors (eigenvectors) that don't change direction when the matrix is applied to them‚Äîthey only scale by a constant (eigenvalue). This is profound: it tells us the intrinsic directions that the matrix "respects." Understanding this is key to understanding matrix behavior.
      </div>

      <div class="visual-example">
If M is a matrix and v is an eigenvector with eigenvalue Œª:
M √ó v = Œª √ó v

Example:
M = [3 0]    v = [1]    Œª = 3
    [0 2]        [0]

M √ó v = [3 0] √ó [1] = [3]   = 3 √ó [1]
        [0 2]   [0]   [0]       [0]

So v is eigenvector with eigenvalue 3!
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Principal Component Analysis (PCA) finds eigenvectors of covariance matrix. Data compression, dimensionality reduction, analyzing neural network hidden states.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Eigenvectors are the natural "directions" for a transformation. They reveal what the matrix does fundamentally. PCA uses largest eigenvectors to find most important dimensions in data.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">For dimensionality reduction and data compression. When analyzing what features your model has learned. When investigating covariance structure of data.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- MATRIX RANK AND NORMS -->
  <section>
    <div class="section-label">Matrix Characteristics</div>
    <h2 class="section-title">Rank, Norms, and Vector Spaces</h2>

    <div class="concept-card" style="--card-accent: var(--green);">
      <div class="concept-title">üìä Matrix Rank</div>
      <div class="concept-description">
        The rank of a matrix tells you how many independent rows or columns it has. It's a measure of "how much information" is in the matrix. A low-rank matrix has redundancy; a full-rank matrix has no redundancy.
      </div>

      <div class="visual-example">
Low rank matrix (rank 1):
[1 2 3]     - Row 2 is 2√ó Row 1
[2 4 6]     - Row 3 is 3√ó Row 1
[3 6 9]
All rows are just multiples of each other!

Full rank matrix (rank 2):
[1 2]       - No redundancy
[3 4]       - Cannot express one row as
[5 6]         multiple of other
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Low-rank matrix factorization for compression. Identifying information bottlenecks in neural networks. Checking if system of equations has solutions.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Rank tells you how many dimensions the transformation actually uses. Neural networks naturally learn low-rank approximations to reduce parameters and computation.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">When analyzing hidden layer representations. When trying to compress models. When checking mathematical properties of weight matrices.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">üìè Vector and Matrix Norms</div>
      <div class="concept-description">
        Norms measure the magnitude or "size" of vectors and matrices. They're used to regularize neural networks (prevent weights from growing too large), evaluate convergence, and measure distances between points.
      </div>

      <div class="visual-example">
L2 Norm (Euclidean distance):
||v|| = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤)
For v = [3, 4]: ||v|| = ‚àö(9+16) = 5

L1 Norm (Manhattan distance):
||v||‚ÇÅ = |v‚ÇÅ| + |v‚ÇÇ| + ... + |v‚Çô|
For v = [3, 4]: ||v||‚ÇÅ = 3 + 4 = 7

L‚àû Norm (Maximum component):
||v||‚àû = max(|v‚ÇÅ|, |v‚ÇÇ|, ..., |v‚Çô|)
For v = [3, 4]: ||v||‚àû = 4
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">L2 regularization (weight decay) in neural networks. L1 regularization for sparsity. Gradient clipping to prevent exploding gradients. Computing distances in embedding spaces.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Norms quantify magnitude. L2 norm is smooth (good for optimization). L1 norm creates sparse solutions (some weights exactly zero). Essential for controlling model complexity.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Every time you train a neural network (regularization). When measuring distances or similarities. When implementing gradient clipping or batch normalization.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- DECOMPOSITION -->
  <section id="decomposition">
    <div class="section-label">Matrix Factorization</div>
    <h2 class="section-title">Matrix Decomposition Methods</h2>

    <div class="concept-card" style="--card-accent: var(--accent);">
      <div class="concept-title">üîÄ SVD: Singular Value Decomposition</div>
      <div class="concept-description">
        SVD breaks any matrix into three simpler matrices: U, Œ£, V^T. This is arguably the most important decomposition in machine learning. It reveals the underlying structure of any matrix and is used for dimensionality reduction, compression, noise removal, and much more.
      </div>

      <div class="visual-example">
Any matrix M can be written as:
M = U √ó Œ£ √ó V^T

Where:
- U: orthogonal matrix (columns are orthonormal)
- Œ£: diagonal matrix with singular values (always ‚â• 0)
- V^T: transpose of orthogonal matrix

The singular values tell you how important each component is.
Keep largest ones, discard small ones = dimensionality reduction!
      </div>

      <div class="info-box">
        <div class="info-box-title">Practical Interpretation</div>
        <div class="info-box-content">Think of SVD as finding the best lower-rank approximation to your matrix. If your matrix has 1000 dimensions but only 10 singular values are significant, you can represent it with 10 dimensions. This is compression.</div>
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">PCA for dimensionality reduction, image compression, recommender systems (matrix factorization), solving least squares problems, understanding hidden representations.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">SVD reveals the true rank and structure of data. It finds the best low-rank approximation, which is why it's used for compression and denoising. It's also numerically stable.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">When you need to understand matrix structure. When compressing data or models. When analyzing what your neural network has learned. When solving inverse problems.</div>
          </div>
        </div>
      </div>
    </div>

    <div class="concept-card" style="--card-accent: var(--warm);">
      <div class="concept-title">üèóÔ∏è QR Decomposition</div>
      <div class="concept-description">
        QR decomposition factors a matrix into an orthogonal matrix Q (columns are perpendicular) and an upper triangular matrix R. It's used for solving linear systems efficiently and for numerical stability in many algorithms.
      </div>

      <div class="visual-example">
M = Q √ó R

Where:
Q is orthogonal: Q^T √ó Q = Identity (columns are perpendicular)
R is upper triangular (all values below diagonal are zero)

This decomposition is more numerically stable than LU decomposition
and is used in QR-based eigenvalue algorithms.
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Solving least squares problems (fitting lines, regression). Computing eigenvalues stably. Orthogonalization of vectors (Gram-Schmidt process). QR algorithms in eigenvalue computation.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">QR is numerically more stable than other methods. Orthogonal matrices preserve lengths and angles, making Q easier to work with. Essential for numerical algorithms.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--accent);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">When solving least squares regression. When you need orthogonal basis vectors. When implementing numerical algorithms that require stability.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- TENSORS -->
  <section>
    <div class="section-label">Higher Dimensions</div>
    <h2 class="section-title">Tensors and Multi-dimensional Arrays</h2>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">üì¶ Beyond Matrices: Tensors</div>
      <div class="concept-description">
        Tensors are generalizations of vectors and matrices to any number of dimensions. A scalar is a 0-D tensor, a vector is 1-D, a matrix is 2-D. In neural networks, you constantly work with 3-D, 4-D, and higher dimensional tensors: batch of images is 4-D (batch, height, width, channels).
      </div>

      <div class="visual-example">
Scalar (0-D):        5
Vector (1-D):        [1, 2, 3]
Matrix (2-D):        [[1, 2], [3, 4]]
Tensor (3-D):        [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]
4-D Tensor:          Batch of images (32, 224, 224, 3)
                     32 images, 224√ó224 pixels, 3 color channels

Most common in deep learning:
- (batch_size, height, width, channels) for images
- (batch_size, sequence_length, embedding_dim) for text
- (batch_size, sequence_length, num_features) for time series
      </div>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Every neural network framework (PyTorch, TensorFlow) uses tensors. Images are 4-D tensors. Batch processing uses first dimension. Attention scores are 3-D tensors.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Tensors allow efficient batch processing. Processing 32 images at once is more efficient than processing one at a time. Tensor operations are optimized on GPUs.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--warm);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Always when using deep learning frameworks. Understanding tensor dimensions is crucial for debugging shape mismatches. Essential for efficient computation.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <!-- APPLICATIONS -->
  <section id="applications">
    <div class="section-label">Real World Impact</div>
    <h2 class="section-title">Where Linear Algebra Powers AI</h2>

    <div class="timeline">
      <div class="timeline-item">
        <div class="timeline-title">Neural Network Forward Pass</div>
        <div class="timeline-desc">Every neuron computes a dot product between input vector and weight vector. A layer with 100 inputs and 50 outputs is really a 100√ó50 matrix multiplication. Entire network is just chained matrix multiplications.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Backpropagation</div>
        <div class="timeline-desc">Gradient computation is matrix multiplication with transposed weights. Computing dL/dW requires multiplying error vectors with input vectors. The chain rule becomes matrix multiplication chain.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Attention Mechanism</div>
        <div class="timeline-desc">Queries √ó Keys^T gives attention weights (matrix multiplication). Attention weights √ó Values gives output (matrix multiplication). Entire transformer is just organized matrix operations.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Dimensionality Reduction</div>
        <div class="timeline-desc">PCA uses eigendecomposition of covariance matrix. t-SNE optimizes using gradient of matrix operations. UMAP uses graph neural networks (which are matrix operations on adjacency matrices).</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Embedding Spaces</div>
        <div class="timeline-desc">Word2vec learns embeddings by optimizing dot product between similar words. Similarity between embeddings is measured using vector norms and dot products. Entire semantic space is linear algebra.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Regularization</div>
        <div class="timeline-desc">L1 and L2 regularization add norm penalties to loss function. Dropout scales weight vectors. Batch normalization normalizes using vector norms. All derived from linear algebra.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Optimization</div>
        <div class="timeline-desc">Gradient descent updates weights using gradients (which are vectors). Learning rate schedules adjust step sizes. Momentum maintains exponential moving average of gradient vectors.</div>
      </div>

      <div class="timeline-item">
        <div class="timeline-title">Generative Models</div>
        <div class="timeline-desc">GANs train generator matrices to fool discriminator. Diffusion models denoise by optimizing matrix operations. VAEs encode to latent vector and decode back. All linear algebra operations underneath.</div>
      </div>
    </div>

  </section>

</div>

<!-- Footer -->
<footer>
  <p class="footer-text">Linear Algebra ‚Äî The Language of Machine Learning</p>
</footer>

</body>
</html>
