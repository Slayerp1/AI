<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Problem Types and Frameworks ‚Äî Formulating and Solving ML Problems</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Bebas+Neue&family=DM+Mono:wght@400;500&display=swap');
@import url('https://fonts.googleapis.com/css2?family=Syne:wght@400;600;700;800&family=Syne+Mono&display=swap');

:root {
  --bg: #04040a;
  --accent: #7b2fff;
  --hot: #ff2d78;
  --cool: #00e5ff;
  --warm: #ffb800;
  --green: #00ff9d;
  --text: #e0e0f0;
  --muted: #4a4a6a;
  --card: #0a0a14;
  --border: rgba(255,255,255,0.07);
}

* { box-sizing: border-box; margin: 0; padding: 0; }
html { scroll-behavior: smooth; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: 'Syne', sans-serif;
  overflow-x: hidden;
  line-height: 1.8;
}

body::before {
  content: '';
  position: fixed; inset: 0; z-index: 0;
  background:
    radial-gradient(ellipse 100% 60% at 50% 0%, rgba(123,47,255,0.12) 0%, transparent 60%),
    radial-gradient(ellipse 60% 40% at 0% 100%, rgba(0,229,255,0.06) 0%, transparent 60%);
  pointer-events: none;
}

.wrap { position: relative; z-index: 1; max-width: 1100px; margin: 0 auto; padding: 0 28px; }

nav {
  position: sticky; top: 0; z-index: 100;
  background: rgba(4,4,10,0.9);
  backdrop-filter: blur(24px);
  border-bottom: 1px solid var(--border);
  padding: 14px 28px;
  display: flex; align-items: center; gap: 10px; flex-wrap: wrap;
}
.nav-brand { font-family: 'Bebas Neue', sans-serif; font-size: 20px; color: var(--accent); margin-right: auto; letter-spacing: 2px; }
.nav-pill {
  font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 2px;
  padding: 5px 12px; border-radius: 20px; border: 1px solid rgba(123,47,255,0.4);
  color: rgba(123,47,255,0.8); text-decoration: none; transition: all 0.2s;
}
.nav-pill:hover { background: rgba(123,47,255,0.15); border-color: var(--accent); color: #fff; }

.hero { padding: 90px 0 50px; }
.hero-eyebrow {
  font-family: 'Syne Mono', monospace; font-size: 11px; letter-spacing: 4px;
  color: var(--accent); text-transform: uppercase; margin-bottom: 18px;
  display: flex; align-items: center; gap: 12px;
}
.hero-eyebrow::after { content: ''; flex: 1; height: 1px; background: rgba(123,47,255,0.3); }

.hero h1 {
  font-family: 'Bebas Neue', sans-serif;
  font-size: clamp(48px, 10vw, 100px);
  line-height: 0.92; margin-bottom: 28px;
  color: #fff;
}

.hero-desc { max-width: 750px; font-size: 16px; color: rgba(210,210,240,0.72); line-height: 1.8; margin-bottom: 40px; }

section { padding: 70px 0; border-top: 1px solid var(--border); }
.section-label { font-family: 'Syne Mono', monospace; font-size: 10px; letter-spacing: 4px; color: var(--accent); text-transform: uppercase; margin-bottom: 16px; }
.section-title { font-family: 'Bebas Neue', sans-serif; font-size: clamp(36px, 6vw, 64px); line-height: 1; margin-bottom: 40px; color: #fff; }

.concept-card {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 16px;
  padding: 40px;
  margin-bottom: 36px;
  border-left: 4px solid var(--card-accent, var(--accent));
  transition: all 0.3s;
}

.concept-card:hover {
  border-color: var(--card-accent, var(--accent));
  background: linear-gradient(135deg, rgba(255,255,255,0.02) 0%, transparent 100%);
  transform: translateY(-2px);
}

.concept-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 28px;
  color: var(--card-accent, var(--accent));
  margin-bottom: 20px;
  letter-spacing: 1px;
}

.concept-body {
  font-size: 15px;
  color: rgba(210,210,240,0.88);
  line-height: 1.9;
  margin-bottom: 28px;
}

.concept-body p {
  margin-bottom: 18px;
}

.concept-body strong {
  color: #fff;
}

.code-block {
  background: rgba(0,0,0,0.5);
  border: 1px solid rgba(0,229,255,0.2);
  border-radius: 12px;
  padding: 24px;
  margin: 28px 0;
  font-family: 'DM Mono', monospace;
  font-size: 13px;
  color: #fff;
  overflow-x: auto;
  line-height: 1.6;
}

.code-comment { color: #5c7a8a; }
.code-keyword { color: var(--accent); }
.code-string { color: var(--green); }
.code-number { color: var(--warm); }
.code-function { color: var(--cool); }

.teaching-box {
  background: rgba(123,47,255,0.1);
  border-left: 4px solid var(--accent);
  padding: 24px;
  border-radius: 10px;
  margin: 28px 0;
  font-size: 15px;
  color: rgba(210,210,240,0.9);
  line-height: 1.8;
}

.teaching-box strong {
  color: #fff;
}

.comparison-box {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
  gap: 20px;
  margin: 28px 0;
}

.comparison-card {
  background: rgba(0,0,0,0.3);
  border: 1px solid rgba(255,255,255,0.08);
  border-radius: 12px;
  padding: 24px;
  border-left: 3px solid var(--comp-color, var(--accent));
}

.comp-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 16px;
  color: var(--comp-color, var(--accent));
  margin-bottom: 14px;
  letter-spacing: 1px;
}

.comp-content {
  font-size: 13px;
  color: rgba(200,200,220,0.85);
  line-height: 1.8;
}

.impact-box {
  background: linear-gradient(135deg, rgba(123,47,255,0.15) 0%, rgba(0,229,255,0.08) 100%);
  border: 1px solid rgba(123,47,255,0.3);
  border-radius: 14px;
  padding: 32px;
  margin-top: 32px;
}

.impact-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
  gap: 28px;
}

.impact-item {
  padding: 24px;
  background: rgba(0,0,0,0.2);
  border-radius: 10px;
  border-left: 4px solid var(--impact-color, var(--accent));
}

.impact-item-title {
  font-family: 'Syne Mono', monospace;
  font-size: 12px;
  letter-spacing: 2px;
  color: var(--impact-color, var(--accent));
  text-transform: uppercase;
  margin-bottom: 14px;
  font-weight: 600;
}

.impact-item-content {
  font-size: 13px;
  color: rgba(200,200,220,0.88);
  line-height: 1.8;
}

.timeline {
  position: relative;
  padding: 40px 0;
}

.timeline-item {
  padding: 28px;
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 10px;
  margin-bottom: 24px;
  margin-left: 40px;
  position: relative;
}

.timeline-item::before {
  content: '';
  position: absolute;
  left: -32px;
  top: 32px;
  width: 16px;
  height: 16px;
  background: var(--accent);
  border-radius: 50%;
  border: 3px solid var(--bg);
}

.timeline-item::after {
  content: '';
  position: absolute;
  left: -24px;
  top: 48px;
  width: 2px;
  height: 44px;
  background: rgba(123,47,255,0.3);
}

.timeline-item:last-child::after {
  display: none;
}

.timeline-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 18px;
  color: var(--cool);
  margin-bottom: 10px;
  letter-spacing: 1px;
}

.timeline-desc {
  font-size: 14px;
  color: rgba(200,200,220,0.85);
  line-height: 1.8;
}

.insight-box {
  background: var(--card);
  border: 1px solid var(--border);
  border-radius: 12px;
  padding: 32px;
  margin: 28px 0;
  border-left: 4px solid var(--insight-color, var(--accent));
}

.insight-title {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 20px;
  color: var(--insight-color, var(--accent));
  margin-bottom: 16px;
  letter-spacing: 1px;
}

.insight-content {
  font-size: 15px;
  color: rgba(210,210,240,0.88);
  line-height: 1.8;
}

footer {
  padding: 60px 0 40px;
  border-top: 1px solid var(--border);
  text-align: center;
}

.footer-text {
  font-family: 'Syne Mono', monospace;
  font-size: 11px;
  letter-spacing: 2px;
  color: var(--muted);
  text-transform: uppercase;
}

@media (max-width: 768px) {
  .hero { padding: 60px 0 40px; }
  section { padding: 50px 0; }
  .impact-grid { grid-template-columns: 1fr; }
  .comparison-box { grid-template-columns: 1fr; }
}
</style>
</head>
<body>

<nav>
  <div class="nav-brand">PROBLEM TYPES</div>
  <a href="#regression" class="nav-pill">Regression</a>
  <a href="#classification" class="nav-pill">Classification</a>
  <a href="#specialized" class="nav-pill">Specialized</a>
</nav>

<div class="wrap">

  <section class="hero">
    <div class="hero-eyebrow">Different Problems, Different Solutions</div>
    <h1>Problem Types and Frameworks</h1>
    <p class="hero-desc">Not all machine learning problems are created equal. The formulation of your problem‚Äîwhat you're trying to predict and under what constraints‚Äîfundamentally determines which algorithms are appropriate, which metrics matter, and what success looks like. A problem predicting continuous house prices is fundamentally different from predicting whether an email is spam. A problem where all classes appear equally in data differs critically from problems where one class is ninety-nine percent of examples. A problem with plenty of labeled training data differs from one where you have only a handful of examples. Understanding problem types transforms you from someone who applies generic algorithms to someone who formulates problems appropriately and selects solutions thoughtfully. This section teaches you to recognize different problem structures, understand their unique challenges, and know which frameworks and techniques apply to each. This knowledge is what separates professionals who build effective systems from generalists who apply the same approach to every problem regardless of fit.</p>
  </section>

  <section id="regression">
    <div class="section-label">Predicting Continuous Values</div>
    <h2 class="section-title">Regression: Learning Continuous Relationships</h2>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">üìà Understanding Regression</div>
      <div class="concept-body">
        <p>Regression is predicting continuous numerical values. When you predict a house price, you're not choosing from discrete categories like "cheap," "medium," or "expensive." You're predicting an actual number: 450,000 dollars. When you predict tomorrow's stock price, you predict a specific value. When you estimate how long a manufacturing process takes, you estimate a continuous duration. The output of a regression model is a number that can theoretically take any value within a range. This fundamental difference from classification shapes everything about how regression models work and how you evaluate them.</p>

        <p>The power of regression is its directional precision. If you predict a house will sell for 500,000 dollars but it actually sells for 520,000, you're close and can quantify exactly how far off you were. This allows for rich evaluation metrics that capture not just whether you're right or wrong, but how wrong. Mean squared error‚Äîthe average of squared prediction errors‚Äîpenalizes large errors more than small ones, reflecting the intuition that being off by 100,000 is worse than being off by 10,000. Mean absolute error measures average absolute error magnitude without penalizing large errors as heavily. These metrics provide nuanced evaluation that binary right-or-wrong classification metrics cannot.</p>

        <p>Understanding what a regression model actually outputs is crucial. Linear regression produces a single predicted value for each input. Neural networks for regression also produce single values but through nonlinear combinations of inputs. Ensemble methods like random forests average predictions across many trees to get a final continuous prediction. The algorithms differ in their approach, but the goal remains the same: mapping inputs to continuous outputs that are as accurate as possible. The challenge is that the continuous nature means there are infinitely many possible correct answers, making the learning problem theoretically harder than classification where you're choosing among a finite set of classes.</p>
      </div>

      <div class="teaching-box">
        Think of regression as learning a function that maps inputs to outputs in a continuous space. A weather model takes current conditions and predicts tomorrow's temperature‚Äîa continuous value. A demand forecasting model takes past sales patterns and predicts future sales‚Äîa continuous quantity. The model must learn smooth relationships where slight input changes produce slight output changes. This is fundamentally different from classification where inputs belong to discrete categories. Regression models learn the shape of the relationship between variables, not discrete boundaries between classes.
      </div>

      <div class="code-block">
<span class="code-comment"># Regression: predicting continuous values</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LinearRegression
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestRegressor
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error, mean_absolute_error, r2_score
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Example: predicting house prices (continuous output)</span>
X_train = ...  <span class="code-comment"># Features like size, bedrooms, age</span>
y_train = ...  <span class="code-comment"># Actual prices: continuous values</span>
X_test = ...
y_test = ...

<span class="code-comment"># Train regression model</span>
model = LinearRegression()
model.fit(X_train, y_train)

<span class="code-comment"># Make predictions: continuous values</span>
y_pred = model.predict(X_test)
<span class="code-comment"># y_pred might be: [450000.5, 680000.2, 320000.8, ...] actual numbers</span>

<span class="code-comment"># Evaluate regression with metrics that capture prediction magnitude</span>
mse = mean_squared_error(y_test, y_pred)
<span class="code-comment"># MSE penalizes large errors: (500k-450k)^2 is much worse than (500k-490k)^2</span>

mae = mean_absolute_error(y_test, y_pred)
<span class="code-comment"># MAE: average absolute error regardless of direction</span>

r2 = r2_score(y_test, y_pred)
<span class="code-comment"># R¬≤ shows what proportion of variance is explained (0 to 1)</span>

<span class="code-function">print</span>(<span class="code-string">f"MAE: ${mae:,.0f}"</span>)  <span class="code-comment"># More interpretable: average error in dollars</span>
<span class="code-function">print</span>(<span class="code-string">f"R¬≤ Score: {r2:.3f}"</span>)  <span class="code-comment"># Model explains R¬≤ of price variation</span>

<span class="code-comment"># Compare with ensemble regression</span>
forest_model = RandomForestRegressor(n_estimators=<span class="code-number">100</span>)
forest_model.fit(X_train, y_train)
forest_pred = forest_model.predict(X_test)

forest_r2 = r2_score(y_test, forest_pred)
<span class="code-function">print</span>(<span class="code-string">f"Random Forest R¬≤: {forest_r2:.3f}"</span>)
<span class="code-comment"># Compare model performances using same metrics</span>
      </code-block>

      <div class="impact-box">
        <div class="impact-grid">
          <div class="impact-item" style="--impact-color: var(--cool);">
            <div class="impact-item-title">üéØ Where It's Used</div>
            <div class="impact-item-content">Predicting prices, stock values, temperatures, demand quantities, durations, amounts. Any problem where the output is a meaningful continuous number rather than a discrete category. Regression is one of the most common problem types in practice.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--green);">
            <div class="impact-item-title">üí° Why It Matters</div>
            <div class="impact-item-content">Understanding that you have a regression problem‚Äînot classification‚Äîshapes everything. You use different evaluation metrics that capture prediction magnitude. You use algorithms designed for continuous outputs. You interpret results differently. Misidentifying a problem as classification when it's regression leads to inappropriate algorithms and misleading evaluations.</div>
          </div>
          <div class="impact-item" style="--impact-color: var(--hot);">
            <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
            <div class="impact-item-content">Whenever predicting a continuous quantity. Identify regression problems by asking: is the output a meaningful continuous number or a discrete category? If it's continuous, you have a regression problem requiring regression-appropriate techniques and metrics.</div>
          </div>
        </div>
      </div>
    </div>

  </section>

  <section id="classification">
    <div class="section-label">Predicting Discrete Categories</div>
    <h2 class="section-title">Classification: Learning to Categorize</h2>

    <div class="concept-card" style="--card-accent: var(--green);">
      <div class="concept-title">üè∑Ô∏è Binary Classification: Yes or No Decisions</div>
      <div class="concept-body">
        <p>Binary classification is the simplest classification problem: predicting one of two possible classes. Is this email spam or not spam? Is this customer likely to churn or stay? Does this medical test result indicate disease or not? The output is always one of exactly two categories. This simplicity makes binary classification mathematically tractable and widely used in practice. Many problems that seem complex at first reduce to binary classification: should we approve this loan or not, is this image of a cat or dog, will this person like this product or not.</p>

        <p>The mathematics of binary classification often involves computing a score or probability for one class, then assigning to that class if the score exceeds a threshold. Logistic regression computes the probability of class 1, assigning to class 1 if probability exceeds 0.5. Neural networks output a probability through a sigmoid function. Support vector machines compute a decision boundary and assign based on which side an example falls. Despite different mathematical approaches, the fundamental idea remains: map inputs to a probability or score for one class, then threshold to make the binary decision.</p>

        <p>The key insight about binary classification is that the choice of threshold affects the tradeoff between true positive rate (correctly identifying class 1) and false positive rate (incorrectly classifying class 0 as class 1). With a very low threshold, nearly everything gets classified as class 1, giving high true positive rate but many false positives. With a high threshold, only very confident predictions become class 1, giving low false positives but missing many true positives. The receiver operating characteristic (ROC) curve visualizes this tradeoff, showing performance across all possible thresholds. The area under the ROC curve (AUC) quantifies overall binary classification performance independent of threshold choice.</p>
      </div>

      <div class="teaching-box">
        Think of binary classification like a judge making a yes-or-no decision. The judge hears evidence and must decide: guilty or not guilty. Real courts struggle with this because they must set a threshold for the evidence needed to convict. Too low a threshold (easily convicted) catches criminals but also convicts innocents. Too high a threshold protects innocents but lets criminals free. This is the true positive vs. false positive tradeoff. Machine learning binary classifiers face the same dilemma: should we be aggressive (high true positive rate) or conservative (low false positive rate)? The answer depends on the costs of each mistake, which changes by problem. The ROC curve visualizes this entire tradeoff space.
      </div>

      <div class="code-block">
<span class="code-comment"># Binary classification: spam detection example</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LogisticRegression
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> confusion_matrix, roc_curve, auc, accuracy_score
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Binary classification: class 0 (not spam) or class 1 (spam)</span>
X_train = ...  <span class="code-comment"># Email features</span>
y_train = ...  <span class="code-comment"># Binary: 0 or 1</span>
X_test = ...
y_test = ...

<span class="code-comment"># Train binary classifier</span>
model = LogisticRegression()
model.fit(X_train, y_train)

<span class="code-comment"># Predictions: binary class assignments</span>
y_pred = model.predict(X_test)  <span class="code-comment"># Returns 0 or 1</span>

<span class="code-comment"># Get probability predictions (scores before thresholding)</span>
y_proba = model.predict_proba(X_test)[:, 1]  <span class="code-comment"># Probability of class 1</span>

<span class="code-comment"># Evaluate binary classification</span>
accuracy = accuracy_score(y_test, y_pred)
<span class="code-comment"># Accuracy: (TP + TN) / total (but can be misleading with imbalance)</span>

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
<span class="code-comment"># True positives: correctly identified spam</span>
<span class="code-comment"># False positives: non-spam marked as spam (bad user experience)</span>
<span class="code-comment"># False negatives: spam marked as not spam (security issue)</span>
<span class="code-comment"># True negatives: correctly identified non-spam</span>

tpr = tp / (tp + fn)  <span class="code-comment"># True positive rate: how much spam do we catch?</span>
fpr = fp / (fp + tn)  <span class="code-comment"># False positive rate: how often do we misclassify?</span>

<span class="code-function">print</span>(<span class="code-string">f"Accuracy: {accuracy:.3f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"True Positive Rate: {tpr:.3f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"False Positive Rate: {fpr:.3f}"</span>)

<span class="code-comment"># ROC curve: evaluate across all thresholds</span>
fpr_curve, tpr_curve, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr_curve, tpr_curve)
<span class="code-function">print</span>(<span class="code-string">f"ROC AUC: {roc_auc:.3f}"</span>)
<span class="code-comment"># AUC 0.5: random guessing. AUC 1.0: perfect classification</span>
      </code-block>
    </div>

    <div class="concept-card" style="--card-accent: var(--warm);">
      <div class="concept-title">üéØ Multi-class and Multi-label Classification</div>
      <div class="concept-body">
        <p>Multi-class classification extends binary classification to more than two categories. Recognizing handwritten digits classifies images into ten classes (0-9). Identifying animal species classifies images into dozens of categories. Sentiment analysis might classify text as positive, neutral, or negative. Unlike binary classification where you choose between two options, multi-class requires choosing among many mutually exclusive options. The fundamental principle remains the same‚Äîmap inputs to categories‚Äîbut the implementation becomes more complex. The model must learn to distinguish among many decision boundaries rather than just one.</p>

        <p>Most classification algorithms naturally extend to multi-class. Logistic regression computes probabilities for each class and assigns to the highest. Decision trees create hierarchical decision boundaries separating classes. Neural networks with softmax output layer compute probabilities across all classes. Support vector machines can use one-vs-rest strategies, training multiple binary classifiers. Despite these implementation details, the core idea persists: partition input space into regions for each class and assign based on which region an input falls into.</p>

        <p>Multi-label classification is different from multi-class in a crucial way. In multi-class, each input has exactly one correct label. An image is either a cat or dog or bird, not multiple simultaneously. In multi-label classification, an input can belong to multiple categories simultaneously. A movie can be both action and romance. A document can discuss multiple topics. A patient might have multiple diagnoses. This changes the problem fundamentally because the output is no longer a single label but a set of labels. The algorithms, evaluation metrics, and interpretation differ substantially. You cannot simply apply multi-class algorithms to multi-label problems and expect good results.</p>
      </div>

      <div class="code-block">
<span class="code-comment"># Multi-class classification: digit recognition</span>
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestClassifier
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> confusion_matrix, classification_report

<span class="code-comment"># Multi-class: 10 possible classes (digits 0-9)</span>
X_train = ...  <span class="code-comment"># Image features</span>
y_train = ...  <span class="code-comment"># Class labels: 0, 1, 2, ..., 9</span>
X_test = ...
y_test = ...

<span class="code-comment"># Multi-class classifier automatically handles all classes</span>
model = RandomForestClassifier(n_classes=<span class="code-number">10</span>)
model.fit(X_train, y_train)

<span class="code-comment"># Predictions assign to one of 10 classes</span>
y_pred = model.predict(X_test)

<span class="code-comment"># Classification report shows metrics per class</span>
<span class="code-function">print</span>(classification_report(y_test, y_pred))
<span class="code-comment"># Shows precision, recall, f1-score for each digit</span>

<span class="code-comment"># Multi-label classification: tag prediction</span>
<span class="code-keyword">from</span> sklearn.multioutput <span class="code-keyword">import</span> MultiOutputClassifier
<span class="code-keyword">from</span> sklearn.tree <span class="code-keyword">import</span> DecisionTreeClassifier

<span class="code-comment"># Multi-label: each document has multiple tags (binary per tag)</span>
X_train = ...  <span class="code-comment"># Document features</span>
y_train = ...  <span class="code-comment"># Binary matrix: shape (n_samples, n_tags)</span>
<span class="code-comment"># Example: [[1, 0, 1], [0, 1, 1], ...] = document tagged with tags 0&2, 1&2, ...</span>

<span class="code-comment"># Multi-label classifier: multiple independent binary classifiers</span>
model = MultiOutputClassifier(DecisionTreeClassifier())
model.fit(X_train, y_train)

<span class="code-comment"># Predictions: binary per tag</span>
y_pred = model.predict(X_test)  <span class="code-comment"># Binary matrix same shape as y_test</span>
<span class="code-comment"># Each example can have any subset of tags</span>
      </code-block>
    </div>

    <div class="impact-box">
      <div class="impact-grid">
        <div class="impact-item" style="--impact-color: var(--green);">
          <div class="impact-item-title">üéØ Where It's Used</div>
          <div class="impact-item-content">Classification problems are ubiquitous. Email filtering, spam detection, disease diagnosis, sentiment analysis, image recognition, document categorization. Any problem where you're assigning inputs to categories rather than predicting continuous values.</div>
        </div>
        <div class="impact-item" style="--impact-color: var(--warm);">
          <div class="impact-item-title">üí° Why It Matters</div>
          <div class="impact-item-content">Classification problems require different evaluation metrics than regression. Binary classification has unique considerations about threshold choice and the true positive vs. false positive tradeoff. Multi-class and multi-label are fundamentally different problems requiring appropriate algorithms and metrics. Misidentifying which type of classification problem you have leads to wrong algorithm choices and invalid evaluation.</div>
        </div>
        <div class="impact-item" style="--impact-color: var(--cool);">
          <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
          <div class="impact-item-content">Identify classification problems by asking: am I assigning inputs to categories? For binary, do I have two classes? For multi-class, are classes mutually exclusive? For multi-label, can inputs have multiple labels simultaneously? The answer determines which algorithms and evaluation approaches apply.</div>
        </div>
      </div>
    </div>

  </section>

  <section id="specialized">
    <div class="section-label">Beyond Standard Problem Formulations</div>
    <h2 class="section-title">Specialized Learning Problems and Frameworks</h2>

    <div class="concept-card" style="--card-accent: var(--hot);">
      <div class="concept-title">‚öñÔ∏è Imbalanced Datasets and Cost-Sensitive Learning</div>
      <div class="concept-body">
        <p>Imbalanced datasets are common in practice: in fraud detection, fraudulent transactions are rare, maybe one percent of all transactions. In disease diagnosis, the disease might affect only a small percentage of the population. In rare event prediction, by definition the event you're predicting is uncommon. Standard classification algorithms struggle with imbalanced data because they optimize overall accuracy, which is misleading when classes are imbalanced. If ninety-nine percent of transactions are legitimate, a model that simply predicts "legitimate" for everything achieves ninety-nine percent accuracy while failing completely at the actual task of catching fraud.</p>

        <p>Imbalanced data creates two related problems. First, the minority class gets underrepresented during training, so the model learns less about it. Second, evaluation metrics that work well with balanced data become misleading with imbalance. Accuracy is not appropriate because it's dominated by the majority class. Precision and recall become more meaningful: precision asks "of the examples we predicted as positive, how many were actually positive?" while recall asks "of all the positive examples, how many did we correctly identify?" These metrics tell you about minority class performance rather than hiding it in overall accuracy.</p>

        <p>Several approaches address imbalance. Resampling techniques oversample the minority class (duplicate examples) or undersample the majority class (remove examples) to balance class representation. Algorithmic approaches like cost-sensitive learning assign higher costs to minority class errors, making the algorithm penalize minority class misclassifications more heavily. Ensemble methods that combine multiple classifiers can learn about rare classes more effectively. The best approach depends on your specific problem, data size, and the cost of different types of errors. A fraud detection system might care much more about catching fraud than avoiding false alarms, while a medical screening test might prioritize avoiding false positives that cause unnecessary treatment.</p>
      </div>

      <div class="teaching-box">
        Think of imbalanced learning like a doctor screening for a rare disease. If the disease affects one percent of patients and the doctor simply tells everyone "you're healthy," they achieve ninety-nine percent accuracy while completely missing the disease in everyone who has it. The doctor needs to be especially attentive to the rare disease cases, potentially using tests that are more sensitive even if they generate more false alarms. Similarly, machine learning algorithms need to weight rare classes appropriately rather than being distracted by majority class dominance.
      </div>

      <div class="code-block">
<span class="code-comment"># Handling imbalanced datasets</span>
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestClassifier
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> precision_score, recall_score, f1_score
<span class="code-keyword">from</span> imblearn.over_sampling <span class="code-keyword">import</span> RandomOverSampler
<span class="code-keyword">from</span> imblearn.under_sampling <span class="code-keyword">import</span> RandomUnderSampler

<span class="code-comment"># Imbalanced data: 99% class 0, 1% class 1</span>
X_train = ...
y_train = ...  <span class="code-comment"># 10000 samples: 9900 class 0, 100 class 1</span>

<span class="code-comment"># Naive approach: standard classifier (will ignore minority)</span>
model_naive = RandomForestClassifier()
model_naive.fit(X_train, y_train)
<span class="code-comment"># Predicts mostly class 0 because it dominates</span>

<span class="code-comment"># Approach 1: Cost-sensitive learning (weight minority class higher)</span>
model_weighted = RandomForestClassifier(class_weight=<span class="code-string">'balanced'</span>)
<span class="code-comment"># Automatically adjusts weights: minority class gets higher penalty</span>
model_weighted.fit(X_train, y_train)

<span class="code-comment"># Approach 2: Oversampling minority class</span>
oversampler = RandomOverSampler(random_state=<span class="code-number">42</span>)
X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)
<span class="code-comment"># Now balanced: 9900 samples each class</span>

model_oversampled = RandomForestClassifier()
model_oversampled.fit(X_train_resampled, y_train_resampled)

<span class="code-comment"># Approach 3: Undersampling majority class</span>
undersampler = RandomUnderSampler(random_state=<span class="code-number">42</span>)
X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)
<span class="code-comment"># Now smaller but balanced: 200 samples total (100 each class)</span>

model_undersampled = RandomForestClassifier()
model_undersampled.fit(X_train_under, y_train_under)

<span class="code-comment"># Evaluate: use precision and recall, not accuracy</span>
y_pred = model_weighted.predict(X_test)

precision = precision_score(y_test, y_pred)  <span class="code-comment"># Of predicted positive, % correct</span>
recall = recall_score(y_test, y_pred)        <span class="code-comment"># Of actual positive, % found</span>
f1 = f1_score(y_test, y_pred)                <span class="code-comment"># Harmonic mean of precision and recall</span>

<span class="code-function">print</span>(<span class="code-string">f"Precision: {precision:.3f} (avoid false alarms)"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Recall: {recall:.3f} (catch actual positives)"</span>)
<span class="code-function">print</span>(<span class="code-string">f"F1 Score: {f1:.3f} (balanced metric)"</span>)
      </code-block>
    </div>

    <div class="concept-card" style="--card-accent: var(--accent);">
      <div class="concept-title">üå± Few-Shot and One-Shot Learning</div>
      <div class="concept-body">
        <p>Few-shot learning addresses a constraint most machine learning assumes away: you have plenty of labeled training data. In many real scenarios, you don't. A pharmaceutical company discovers a new disease and might have only handful of patient cases. A security system encounters a new type of fraud with few known examples. A robot needs to manipulate new objects it has never seen before. Standard machine learning approaches require thousands of examples to learn well. Few-shot learning asks: can we learn from just a few examples, even one?</p>

        <p>Few-shot learning exploits meta-learning: learning how to learn. Rather than training on a specific task with many examples, meta-learning trains on many diverse tasks with few examples each, learning a learning procedure that generalizes to new tasks. A face recognition system might be meta-trained on thousands of tasks where each task is "recognize whether these two faces belong to the same person," using only a few examples per task. After this meta-training, the system can solve new face recognition tasks with just one or a few examples of the new person.</p>

        <p>One-shot learning, the extreme case, learns from a single example. Given one image of a new object, can the system recognize that object in future images? This seems impossible in traditional machine learning‚Äîa single example cannot establish a pattern. Yet humans do this routinely: a child sees a new animal once and recognizes it again. The key is transfer learning and meta-learning. The model learns from many tasks to build representations where similarity is meaningful. A new example defines a point in this learned space, and the model recognizes similar points as the same category. One-shot learning is not truly learning from one example in isolation but rather leveraging learned representations from massive prior learning.</p>
      </div>

      <div class="teaching-box">
        Few-shot and one-shot learning mimic how humans actually learn. A human learns what a zebra is from potentially just a few images, then recognizes zebras in new contexts. A human learns a new game from watching someone play once or twice, then plays competently. This is radically different from the way standard machine learning works, which requires thousands of examples. Few-shot learning asks: can we build systems that learn more like humans, extracting knowledge from limited examples by leveraging prior learning on related tasks? This is a profound shift in how we think about machine learning.
      </teaching-box>
    </div>

    <div class="concept-card" style="--card-accent: var(--cool);">
      <div class="concept-title">‚è±Ô∏è Online Learning and Streaming Data</div>
      <div class="concept-body">
        <p>Online learning addresses a different constraint: standard machine learning assumes you have all training data available upfront, can process it as much as you want, then deploy a fixed model. Reality is different. Data arrives continuously in streams. A recommendation system must process millions of user interactions per second, each providing signal about what works. A fraud detection system must detect emerging fraud patterns in real-time as they appear. A weather prediction system receives constant sensor data. Standard batch learning‚Äîcollecting all data then training‚Äîis impractical for true streaming scenarios.</p>

        <p>Online learning algorithms process one example at a time, update the model incrementally, then discard the example. This one-pass approach is computationally efficient and works with unbounded data streams. Stochastic gradient descent, the algorithm that trains neural networks, is fundamentally an online algorithm: it updates model weights after each example (or small batch) rather than waiting for all data. This makes neural networks naturally suited to online learning compared to algorithms like decision trees that need to see all data to build optimal splits.</p>

        <p>Online learning creates unique challenges. You cannot use a separate test set because future examples arrive as part of the stream. You cannot reprocess historical data to improve the model. Concept drift‚Äîwhere the underlying data distribution changes over time‚Äîbecomes a central concern. A spam filter trained months ago might not work as spammers adapt their tactics. An online learning system must adapt continuously, learning new patterns while retaining knowledge of old ones. This balance between stability and plasticity is fundamentally different from the batch learning paradigm taught in most machine learning courses.</p>
      </div>

      <div class="teaching-box">
        Online learning is learning while doing, not learning then doing. A language model that updates as it receives feedback from users is online learning. A recommendation system that adjusts to changing preferences in real-time is online learning. A weather model that incorporates new sensor readings continuously is online learning. This is how human learning actually works: you learn from experiences as they happen, updating your understanding continuously. Standard machine learning's batch paradigm‚Äîcollect data, train once, deploy‚Äîis actually the exception, not the rule.
      </teaching-box>

      <div class="code-block">
<span class="code-comment"># Online learning: updating model as data streams in</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> SGDClassifier
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> SGDRegressor
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Online learning classifier using Stochastic Gradient Descent</span>
<span class="code-comment"># SGD updates after each example (or small batch) rather than waiting for all data</span>
model = SGDClassifier(loss=<span class="code-string">'log_loss'</span>, penalty=<span class="code-string">'l2'</span>)

<span class="code-comment"># Simulate streaming data: data arrives over time</span>
<span class="code-keyword">for</span> batch_idx <span class="code-keyword">in</span> <span class="code-function">range</span>(<span class="code-number">1000</span>):
    <span class="code-comment"># In reality, this batch arrives from a data stream</span>
    X_batch = generate_batch()  <span class="code-comment"># Get next batch of data</span>
    y_batch = get_labels()      <span class="code-comment"># Get corresponding labels</span>
    
    <span class="code-comment"># Update model with this batch (online update)</span>
    model.partial_fit(X_batch, y_batch, classes=[<span class="code-number">0</span>, <span class="code-number">1</span>])
    <span class="code-comment"># partial_fit allows incremental updates without reprocessing old data</span>
    
    <span class="code-comment"># Model is immediately ready to make predictions on new examples</span>
    <span class="code-keyword">if</span> batch_idx % <span class="code-number">100</span> == <span class="code-number">0</span>:
        test_batch = get_test_batch()
        score = model.score(test_batch[<span class="code-string">'X'</span>], test_batch[<span class="code-string">'y'</span>])
        <span class="code-function">print</span>(<span class="code-string">f"Batch {batch_idx}: Score = {score:.3f}"</span>)
        <span class="code-comment"># Score changes as model learns from streaming data</span>

<span class="code-comment"># Compare with batch learning (traditional)</span>
<span class="code-comment"># With batch: collect all data, train once, deploy</span>
<span class="code-comment"># With online: continuously update as data arrives</span>

<span class="code-comment"># Online learning for regression (price prediction)</span>
sgd_reg = SGDRegressor(loss=<span class="code-string">'squared_error'</span>)

<span class="code-comment"># As prices change, update model incrementally</span>
<span class="code-keyword">for</span> new_datapoint <span class="code-keyword">in</span> stream_of_prices:
    X_new = new_datapoint[<span class="code-string">'features'</span>].reshape(<span class="code-number">1</span>, -<span class="code-number">1</span>)
    y_new = new_datapoint[<span class="code-string">'price'</span>]
    sgd_reg.partial_fit(X_new, [y_new])
    <span class="code-comment"># Model adapts to new prices without forgetting learned relationships</span>
      </code-block>
    </div>

    <div class="impact-box">
      <div class="impact-grid">
        <div class="impact-item" style="--impact-color: var(--hot);">
          <div class="impact-item-title">üéØ Where It's Used</div>
          <div class="impact-item-content">Imbalanced data appears everywhere: fraud detection, disease diagnosis, rare event prediction. Few-shot learning is crucial when training examples are expensive. Online learning is necessary for streaming systems: trading algorithms, recommendation systems, real-time analytics.</div>
        </div>
        <div class="impact-item" style="--impact-color: var(--accent);">
          <div class="impact-item-title">üí° Why It Matters</div>
          <div class="impact-item-content">Understanding specialized problem types prevents applying inappropriate standard algorithms. Imbalanced data requires cost-sensitive approaches, not standard accuracy metrics. Few-shot requires meta-learning, not collecting more examples. Online learning requires streaming algorithms, not batch training. Recognizing problem type shapes entire solution architecture.</div>
        </div>
        <div class="impact-item" style="--impact-color: var(--green);">
          <div class="impact-item-title">‚è±Ô∏è When You Need It</div>
          <div class="impact-item-content">When you notice class imbalance, use appropriate metrics and techniques. When you have limited labeled data, consider few-shot or meta-learning approaches. When data arrives continuously, use online learning algorithms. These specialized frameworks address real-world constraints standard ML courses don't cover.</div>
        </div>
      </div>
    </div>

  </section>

  <section>
    <div class="section-label">The Fundamental Principle</div>
    <h2 class="section-title">Problem Type Determines Solution Approach</h2>

    <div class="insight-box" style="--insight-color: var(--cool);">
      <div class="insight-title">Proper Problem Formulation Precedes Algorithm Selection</div>
      <div class="insight-content">The most common mistake in machine learning is jumping to algorithm selection before properly understanding the problem type. Is this regression or classification? If classification, how many classes and are they balanced? Is the output single-label or multi-label? Are you learning from abundant data or limited examples? Is data static or streaming? The answers to these questions determine which algorithms are appropriate far more than raw algorithm sophistication. A simple algorithm applied to a properly formulated problem beats a complex algorithm applied to a misformulated problem every time. Understanding problem types transforms you from someone who applies the same algorithms to everything into someone who formulates problems appropriately and selects solutions thoughtfully. This discrimination is what separates professionals from generalists.</div>
    </div>

    <div class="insight-box" style="--insight-color: var(--green);">
      <div class="insight-title">Evaluation Metrics Must Match Problem Type</div>
      <div class="insight-content">Different problem types require different evaluation metrics. Regression uses MSE, MAE, R¬≤. Binary classification uses accuracy only with balanced data, but precision, recall, and AUC with imbalance. Multi-class uses per-class metrics and confusion matrices. Few-shot learning is evaluated on held-out tasks. Online learning is evaluated on streaming accuracy. Using wrong metrics misleads you about model performance. An imbalanced classification problem evaluated with accuracy looks solved when it's actually failing. A regression problem evaluated with classification metrics is nonsensical. Matching problem type to evaluation metrics ensures your metrics actually measure success rather than misleading you.</div>
    </div>

    <div class="insight-box" style="--insight-color: var(--hot);">
      <div class="insight-title">Real Problems Are Messier Than Textbook Problems</div>
      <div class="insight-content">Textbook machine learning examples are clean: balanced classes, plenty of data, static distributions, clear problem boundaries. Real problems are messier. Data is imbalanced, examples are expensive, distributions drift over time, problem boundaries are ambiguous. Real professionals spend time understanding these messy characteristics and adjusting their approach accordingly. They recognize imbalance and apply cost-sensitive techniques. They notice concept drift and use online learning. They work with limited labeled data and use few-shot or semi-supervised approaches. This pragmatic recognition that problems don't match textbooks is what separates academics who study idealized problems from professionals who solve real ones. The fundamentals you learn about problem types and frameworks are your tools for navigating this messy reality.</div>
    </div>

  </section>

</div>

<footer>
  <p class="footer-text">Problem Types and Frameworks ‚Äî Formulating Solutions Appropriately</p>
</footer>

</body>
</html>
